"""Novel registry and per-novel workspace path management.

This module implements the backend primitives described in
`NOVEL_LIBRARY_DEVELOPMENT_PLAN.md` Phase 1:
- Each novel gets an isolated workspace under `data/novels/<novel_id>/...`
- Each novel gets an isolated local Qdrant directory under `vector_db/<novel_id>/...`
"""

from __future__ import annotations

import json
import os
import re
import secrets
import shutil
import threading
from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional


def _utc_now() -> str:
    return datetime.now(timezone.utc).isoformat()


_SLUG_SAFE_RE = re.compile(r"[^a-z0-9]+")


def _slugify(value: str) -> str:
    value = (value or "").strip().lower()
    value = _SLUG_SAFE_RE.sub("-", value)
    value = value.strip("-")
    return value


def _validate_novel_id(novel_id: str) -> str:
    novel_id = str(novel_id or "").strip()
    if not novel_id:
        raise KeyError("novel_id is empty")
    # Minimal traversal guard (IDs are generated by us, so reject suspicious input).
    if "/" in novel_id or "\\" in novel_id or ".." in novel_id:
        raise KeyError("invalid novel_id")
    return novel_id


def _is_within_dir(path: str, root: str) -> bool:
    """Prevent accidental deletions outside workspace roots."""
    root_abs = os.path.abspath(root)
    path_abs = os.path.abspath(path)
    try:
        common = os.path.commonpath([root_abs, path_abs])
    except ValueError:
        return False
    return common == root_abs


@dataclass
class NovelEntry:
    novel_id: str
    title: str = ""
    status: str = "created"  # created|uploaded|processing|ready|failed|deleted
    created_at: str = field(default_factory=_utc_now)
    updated_at: str = field(default_factory=_utc_now)
    source: Dict[str, Any] = field(default_factory=dict)
    stats: Dict[str, Any] = field(default_factory=dict)
    last_job_id: str = ""
    last_error: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "NovelEntry":
        return cls(
            novel_id=str(data.get("novel_id", "")),
            title=str(data.get("title", "")),
            status=str(data.get("status", "created")),
            created_at=str(data.get("created_at", _utc_now())),
            updated_at=str(data.get("updated_at", _utc_now())),
            source=dict(data.get("source", {}) or {}),
            stats=dict(data.get("stats", {}) or {}),
            last_job_id=str(data.get("last_job_id", "")),
            last_error=str(data.get("last_error", "")),
        )


class NovelRegistry:
    """Filesystem-backed registry for multi-novel management."""

    def __init__(
        self,
        data_root: str,
        vector_db_root: str,
        logs_root: str,
    ):
        self.data_root = data_root
        self.vector_db_root = vector_db_root
        self.logs_root = logs_root
        self._lock = threading.Lock()

        os.makedirs(self._novels_root_dir(), exist_ok=True)
        os.makedirs(self.vector_db_root, exist_ok=True)
        os.makedirs(self.logs_root, exist_ok=True)

    def _novels_root_dir(self) -> str:
        return os.path.join(self.data_root, "novels")

    def _index_path(self) -> str:
        return os.path.join(self._novels_root_dir(), "index.json")

    def _load_index(self) -> Dict[str, Any]:
        path = self._index_path()
        if not os.path.exists(path):
            return {"novels": []}
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, dict):
            return {"novels": []}
        if "novels" not in data or not isinstance(data.get("novels"), list):
            data["novels"] = []
        return data

    def _save_index(self, data: Dict[str, Any]) -> None:
        os.makedirs(self._novels_root_dir(), exist_ok=True)
        with open(self._index_path(), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def list(self) -> List[Dict[str, Any]]:
        with self._lock:
            index = self._load_index()
            entries = [NovelEntry.from_dict(item).to_dict() for item in index.get("novels", [])]
        # Sort by updated time descending for UI convenience.
        entries.sort(key=lambda x: x.get("updated_at", ""), reverse=True)
        return entries

    def get(self, novel_id: str) -> Dict[str, Any]:
        novel_id = _validate_novel_id(novel_id)

        with self._lock:
            index = self._load_index()
            for item in index.get("novels", []):
                entry = NovelEntry.from_dict(item)
                if entry.novel_id == novel_id:
                    return entry.to_dict()
        raise KeyError(f"novel not found: {novel_id}")

    def create(self, title: str = "") -> Dict[str, Any]:
        title = str(title or "").strip()
        slug = _slugify(title)
        if not slug:
            slug = "novel"

        with self._lock:
            index = self._load_index()
            existing = {str(item.get("novel_id", "")) for item in index.get("novels", [])}

            # Avoid collisions.
            for _ in range(50):
                suffix = secrets.token_hex(3)  # 6 chars
                novel_id = f"{slug}-{suffix}"
                if novel_id not in existing:
                    break
            else:
                raise RuntimeError("failed to allocate novel_id after many attempts")

            entry = NovelEntry(novel_id=novel_id, title=title)
            index["novels"].append(entry.to_dict())
            self._save_index(index)

        # Ensure workspace dirs exist.
        paths = self.paths(novel_id)
        os.makedirs(paths["input_dir"], exist_ok=True)
        os.makedirs(paths["chapters_dir"], exist_ok=True)
        os.makedirs(paths["scenes_dir"], exist_ok=True)
        os.makedirs(paths["annotated_dir"], exist_ok=True)
        os.makedirs(paths["profiles_dir"], exist_ok=True)
        os.makedirs(paths["sessions_dir"], exist_ok=True)
        os.makedirs(paths["vector_db_path"], exist_ok=True)
        os.makedirs(paths["log_dir"], exist_ok=True)

        return entry.to_dict()

    def update(self, novel_id: str, **fields: Any) -> Dict[str, Any]:
        novel_id = _validate_novel_id(novel_id)

        with self._lock:
            index = self._load_index()
            updated = None
            novels = index.get("novels", [])
            for idx, item in enumerate(novels):
                entry = NovelEntry.from_dict(item)
                if entry.novel_id != novel_id:
                    continue

                for key, value in fields.items():
                    if key in {"novel_id", "created_at"}:
                        continue
                    if hasattr(entry, key):
                        setattr(entry, key, value)
                entry.updated_at = _utc_now()
                novels[idx] = entry.to_dict()
                updated = entry
                break
            if updated is None:
                raise KeyError(f"novel not found: {novel_id}")
            self._save_index(index)
            return updated.to_dict()

    def delete(self, novel_id: str, delete_vector_db: bool = False) -> None:
        novel_id = _validate_novel_id(novel_id)

        with self._lock:
            index = self._load_index()
            novels = index.get("novels", [])
            kept = []
            found = False
            for item in novels:
                entry = NovelEntry.from_dict(item)
                if entry.novel_id == novel_id:
                    found = True
                    continue
                kept.append(entry.to_dict())
            if not found:
                raise KeyError(f"novel not found: {novel_id}")
            index["novels"] = kept
            self._save_index(index)

        # Delete workspace dirs on disk.
        workspace_dir = os.path.join(self._novels_root_dir(), novel_id)
        if os.path.isdir(workspace_dir) and _is_within_dir(workspace_dir, self._novels_root_dir()):
            shutil.rmtree(workspace_dir, ignore_errors=True)

        if delete_vector_db:
            vdb_dir = os.path.join(self.vector_db_root, novel_id)
            if os.path.isdir(vdb_dir) and _is_within_dir(vdb_dir, self.vector_db_root):
                shutil.rmtree(vdb_dir, ignore_errors=True)

    def paths(self, novel_id: str) -> Dict[str, str]:
        novel_id = _validate_novel_id(novel_id)

        novel_dir = os.path.join(self._novels_root_dir(), novel_id)
        input_dir = os.path.join(novel_dir, "input")
        sessions_dir = os.path.join(novel_dir, "sessions")
        chapters_dir = os.path.join(novel_dir, "chapters")
        scenes_dir = os.path.join(novel_dir, "scenes")
        annotated_dir = os.path.join(novel_dir, "annotated")
        profiles_dir = os.path.join(novel_dir, "profiles")

        vector_db_path = os.path.join(self.vector_db_root, novel_id)
        log_dir = os.path.join(self.logs_root, "novels", novel_id)

        return {
            "novel_dir": novel_dir,
            "input_dir": input_dir,
            "source_file": os.path.join(input_dir, "source.txt"),
            "sessions_dir": sessions_dir,
            "chapters_dir": chapters_dir,
            "scenes_dir": scenes_dir,
            "annotated_dir": annotated_dir,
            "profiles_dir": profiles_dir,
            "vector_db_path": vector_db_path,
            "log_dir": log_dir,
        }
